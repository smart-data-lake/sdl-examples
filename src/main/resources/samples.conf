#
# Smart Data Lake - Build your data lake the smart way.
#
# Copyright Â© 2019 ELCA Informatique SA (<https://www.elca.ch>)
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.
#

connections {

  localHsqlJdbc {
    type = JdbcTableConnection
    url = "jdbc:hsqldb:file:csv-to-jdbc/hsqldb"
    driver = org.hsqldb.jdbcDriver
  }

}

dataObjects {

  custom-my-df {
    type = CustomDfDataObject
    creator {
      class-name = com.sample.CreateMyDf
    }
  }

  custom-csv-entire-room {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options {
      delimiter = "|"
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  ab-csv-org {
    type = CsvFileDataObject
    path = "AB_NYC_2019.csv"
    csv-options {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  ab-csv-hadoop {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options {
      delimiter = "|"
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  ab-parquet-hadoop {
    type = ParquetFileDataObject
    path = "~{id}/data/parquet"
  }

  ab-hive {
    type = HiveTableDataObject
    path = "~{id}/data/hive"
    table {
      db = default
      name = "ab_hive"
      primary-key = [id]
    }
  }

  ab-excel {
    type = ExcelFileDataObject
    path = "~{id}/AB_NYC_2019.xlsx"
    excel-options {
      sheet-name = csvdata
    }
  }

  ab-reduced-hsqldb {
    type = JdbcTableDataObject
    connectionId = localHsqlJdbc
    create-sql = "create table if not exists ~{table.db}.~{table.name} (id int, name varchar(255))"
    table {
      db = public
      name = nyc_reduced
    }
  }

  ab-reduced-csv-hadoop {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  ab-transformed-csv-hadoop {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  ab-multi-transformed-csv-hadoop1 {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  ab-multi-transformed-csv-hadoop2 {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  custom-rating-df {
    type = CustomDfDataObject
    creator {
      class-name = com.sample.CreateRatingDf
    }
  }

  custom-rating-csv1 {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  custom-rating-csv2 {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
  }

  custom-rating-csv-agg {
    type = CsvFileDataObject
    path = "~{id}"
    csv-options = {
      delimiter = ","
      escape = "\\"
      header = "true"
      quote = "\""
    }
    //sparkRepartition { numberOfTasksPerPartition = 1, filename = result.csv }
    sparkRepartition1 {
      number-of-tasks-per-Partition11234 = 1,
      filename = result.csv
    }
    metadata = {
      subject-area = test
    }
  }

}

actions {

  customDf2Csv {
    type = CopyAction
    inputId = custom-my-df
    outputId = custom-csv-entire-room
    metadata {
      feed = custom-my-df2csv
    }
  }

  copyCsv {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-csv-hadoop
    additionalColumns = {
      run_id = runId
    }
    metadata {
      feed = ab-csv
    }
  }

  loadParquet {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-parquet-hadoop
    metadata {
      feed = ab-parquet-hive
    }
  }

  loadHive {
    type = CopyAction
    inputId = ab-parquet-hadoop
    outputId = ab-hive
    metadata {
      feed = ab-parquet-hive
    }
  }

  loadExcel {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-excel
    metadata {
      feed = ab-excel
    }
  }

  loadTransformedWithSQL {
    type = CopyAction
    inputId = ab-csv-org
    outputId =  ab-transformed-csv-hadoop
    transformer {
      sqlCode = "select id,name,host_id,number_of_reviews, '%{test}' as test, %{run_id} as run_id from ab_csv_org where neighbourhood_group = 'Manhattan'"
      options = {
        test = "test * 2" // options are treated as string
      }
      runtimeOptions = {
        run_id = "runId * 2" // runtime options are evaluated as spark SQL expressions against DefaultExpressionData
      }
    }
    metadata {
      feed = ab-sql-transform
    }
  }

  loadTransformedWithMultipleSQLs {
    type = CustomSparkAction
    inputIds = [ab-csv-org]
    outputIds = [ab-multi-transformed-csv-hadoop1,ab-multi-transformed-csv-hadoop2]
    transformer {
      sql-code {
        ab-multi-transformed-csv-hadoop1 = "select id,name,host_id,number_of_reviews from ab_csv_org where neighbourhood_group = 'Manhattan'",
        ab-multi-transformed-csv-hadoop2 = "select id,name from ab_csv_org where neighbourhood_group = 'Brooklyn'"
      }
    }
    metadata {
      feed =  ab-sql-multi-transform
    }
  }

  loadJdbc {
    type = CopyAction
    inputId = ab-csv-org
    outputId = ab-reduced-hsqldb
    transformer.class-name = com.sample.ReduceNycCSVTransformer
    metadata {
      feed = ab-jdbc
    }
  }

  loadJdbc2Csv {
    type = CopyAction
    inputId = ab-reduced-hsqldb
    outputId = ab-reduced-csv-hadoop
    metadata {
      feed = ab-jdbc
    }
  }

  customRatingDf2Csv1 {
    type = CopyAction
    inputId = custom-rating-df
    outputId = custom-rating-csv1
    metadata {
      feed = custom-rating-csv
    }
  }
  customRatingDf2Csv2 {
    type = CopyAction
    inputId = custom-rating-df
    outputId = custom-rating-csv2
    metadata {
      feed = custom-rating-csv
    }
  }
  customRatingAgg {
    type = CustomSparkAction
    inputIds = [custom-rating-csv1, custom-rating-csv2]
    outputIds = [custom-rating-csv-agg]
    transformer.class-name = com.sample.RatingTransformer
    metadata {
      feed = custom-rating-csv
    }
  }

}

